Ringkasan Bab 10: Seni Rekonstruksi Diri (Autoencoders)
Bab ini adalah panduan komprehensif mengenai Autoencoder, sebuah keluarga arsitektur jaringan saraf yang unik. Berbeda dengan model klasifikasi yang memprediksi label, Autoencoder belajar dengan cara memadatkan data dan mencoba mengembalikannya ke bentuk semula (rekonstruksi).

Prinsip kerjanya bergantung pada dua organ utama:
1.	Encoder (Kompresor): Mempersempit data input menjadi representasi padat berdimensi rendah yang disebut vektor laten (z).
2.	Decoder (Rekonstruktor): Mengambil kode padat z tersebut dan mengembangkannya kembali menjadi data yang sebisa mungkin mirip dengan input aslinya.
________________________________________

1. Autoencoder Klasik (Vanilla)
Bagian ini memperkenalkan bentuk paling purba dari autoencoder menggunakan dataset digit MNIST.
•	Arsitektur: Dibangun sepenuhnya menggunakan lapisan Dense (jaringan terhubung penuh).
•	Mekanisme: Model dipaksa melewati "leher botol" (bottleneck)—lapisan tengah dengan jumlah neuron yang sangat sedikit.
•	Tujuan: Dengan meminimalkan selisih (reconstruction loss) antara input dan output, model secara otomatis belajar membuang informasi yang tidak penting dan hanya menyimpan fitur esensial dalam ruang laten.
________________________________________

2. Autoencoder Penghilang Derau (Denoising Autoencoder - DAE)
Bagaimana cara membuat model yang tidak hanya menghafal, tapi benar-benar memahami struktur gambar? Jawabannya adalah dengan mempersulit tugasnya.
•	Tantangan: Kita sengaja merusak gambar input MNIST dengan menambahkan noise (bintik-bintik acak).
•	Tugas: Model diperintahkan untuk menghasilkan output berupa gambar bersih (tanpa noise), bukan gambar input yang rusak.
•	Efek: Ini memaksa Autoencoder untuk menjadi cerdas: ia harus belajar memisahkan antara sinyal (pola angka) dan noise (gangguan), menghasilkan fitur yang jauh lebih tangguh (robust).
________________________________________

3. Autoencoder Konvolusional (CAE)
Untuk data gambar yang lebih kompleks seperti Fashion-MNIST, lapisan Dense tidak lagi efisien. Kita beralih ke operasi konvolusi.

•	Encoder: Menggunakan Conv2D dan MaxPooling2D untuk secara bertahap mengecilkan dimensi spasial gambar (downsampling) menjadi representasi abstrak.
•	Decoder: Menggunakan Conv2DTranspose (sering disebut deconvolution) dan UpSampling2D untuk membesarkan kembali representasi tersebut menjadi citra utuh. Arsitektur ini jauh lebih unggul dalam menjaga integritas spasial gambar.
________________________________________
4. Translasi Citra: U-Net

Bagian terakhir membahas aplikasi tingkat lanjut: mengubah satu jenis gambar menjadi jenis lain (misalnya, mengubah sketsa bangunan menjadi foto realistis) menggunakan dataset pix2pix.

•	Arsitektur U-Net:
Ini adalah evolusi dari Autoencoder standar. Masalah utama autoencoder biasa adalah hilangnya detail halus (tepi, tekstur) karena proses kompresi yang agresif.

•	Solusi Skip Connections:
U-Net menambahkan "jalan pintas" yang menghubungkan lapisan Encoder langsung ke lapisan Decoder yang bersesuaian.
o	Fungsi: Informasi detail resolusi tinggi dari Encoder ditransfer langsung ke Decoder tanpa harus diperas melalui leher botol.
o	Hasil: Decoder dapat menggabungkan pemahaman konteks global (dari lapisan dalam) dengan detail tekstur halus (dari jalan pintas), menghasilkan rekonstruksi gambar yang sangat tajam dan realistis.
